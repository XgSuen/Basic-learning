# 百面机器学习

## 特征工程

结构数据：类似数据库表，有特定类型：数值型和类别型

非结构数据：文本，图像，音频，视频等

### Q1. 特征归一化

**做法**：将所有特征都统一到一个大致相同的数值区间内；

**目的：**消除量纲影响，使得不同指标之间具有可比性；

**方法：**

1. 线性函数归一化（Min-Max scaling），映射到[0,1]范围：
   $$
   X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
   $$
   
2. 零均值归一化（Z-score Normalization），映射到均值为0，方差为1的分布：
   $$
   z=\frac{x-\mu}{\sigma}
   $$

此外，特征归一化可以使得模型学习速率更快，适合梯度下降型的学习算法，对于决策树这种使用信息增益的算法不适用。

### Q2. 类别型特征处理

1. 序号编码：按照一定顺序编号，保留了大小关系
2. one-hot编码：将每个类别编码成类别数量规模大小的向量，在所处位置为1，其余位置为0.（1）稀疏向量节省空间；（2）配合特征选择降维。
3. 二进制编码：先转化为类别ID，再将其转换为二进制编码。维度低，节省空间。

### Q3. 组合特征

提高复杂关系的拟合能力，将一阶离散特征两两组合，构成高阶组合特征。

比如两个类别特征，每个类别有两个取值，那么组合后会变成2x2=4的高阶特征。

### Q4. 有效的找到高阶组合特征

可以采用基于决策树的特征组合方式，从根节点到叶节点的每条路径都可以看成是一种特征组合方式。

> 关于组合特征和组合高阶特征，比如在广告点击预测任务中，找到各个属性（年龄，性别，是否付费等等）相互组合方式对应是否点击的结果来找到行之有效的组合特征，避免过拟合

### Q5. 文本表示模型

1. 词袋模型和TF-IDF

   将每篇文章或每段文本表示成一个向量，先将其**分词**，其长度就是文章或者文本所包含的词数量，向量的值就是对应位置词的权重，可以通过TF-IDF来求得：
   $$
   TF-IDF(t,d)=TF(t,d)\times IDF(t)
   $$
   其中$TF(t,d)$是单词 t 在文档 d 中出现的频率，$IDF(t)$是逆文档频率，衡量单词 t 对表达语义所起的重要性：$IDF(t)=\frac{文章总数}{log(包含单词t的文章总数+1)}$.也就是说，单词t在所有文章中出现次数越多，越通用，就越不重要。

   有时，连续的词被分开含义会发生巨大变化，所以可以将连续出现的 n 个词组成词组（n-gram）也作为一个单独的特征放到向量表示中去。还会对单词进行词干抽取（word stemming），因为词性虽然变化但含义相同，将多种形态的同个词处理为统一的词干形式。

2. 主题模型（Topic model）

   从文本库发现有代表性的主题，得到每个主题上词的分布，从而计算文章的主题分布。

3. 词嵌入模型（Word Embedding）

   将词向量化，将每个词映射为低维空间上的一个稠密向量。

> 在浅层的机器学习模型中，好的特征工程可以带来算法效果的显著提升。
>
> 深度学习模型提供了一种自动特征工程的方式，模型中每个隐层都可以任务对应不同抽象层次的特征。

### Q6. word2vec

词嵌入模型word2vec分为CBOW和Skip-gram两种网络结构，CBOW是通过窗口的上下文预测中心词的生产概率，Skip-gram是通过窗口的中心词预测上下文的生成概率。他们都由输入层，映射层和输出层组成。单词通过one-hot编码表示。

![image-20220514231706189](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220514231706189.png)

**对于CBOW**，给定一个窗口，通过上下文来预测中心词。即先将中心词的上下文单词的初始词向量相加，然后通过变换映射到词库大小的维度，在这个N维的输出向量上使用softmax计算对应每个词库的概率，然后使用N分类交叉熵损失函数更新窗口内的词向量。通常情况下，这个N会非常大，而计算softmax函数要求挨个词计算它们的概率，即复杂度为O(N)，导致速度非常慢。

**一种优化方式是Hierachical（层级）softmax：**

![image-20220514232432032](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220514232432032.png)

层级softmax只需要计算O(logN)次，大大减少了计算复杂度。它的大致过程为：

（1）首先统计N个词的词频，**根据词频建立Huffman树**，其中满足左孩子的词频大于右孩子，**左孩子编码为1，右孩子编码为0**。叶子节点表示所有词，而从根节点通往某个词的路径中所经过的节点都有属于自己的参数向量。（2）然后将窗口上下文词向量的和作为输入，找到根节点到中心词的路径，**根据Huffman编码将每条路径打上标签，0/1，从根节点开始进行二分类**，即向量和与当前节点代表的参数相乘，经过sigmoid函数生成概率值，然后与当前路径编码的标签做二分类损失函数，指导遍历完此路径，到达根节点，将所有损失相加，**最大化该损失和**。（3）然后采用**随机梯度上升法更新词向量**，即每取一个样本（窗口），就对目标函数中所有的参数（路径节点向量）做一次更新。因此要更新两组关键参数：路径中非叶节点的向量参数和窗口内中心词的上下文词向量。让损失函数分别对这两个参数求导得到梯度，再让这些参数加上梯度乘以学习率即可实现梯度上升更新参数。由于我们的输入是词向量和，对每个词向量更新时一般就直接将对词向量和梯度贡献到每个上下文词向量中。

可以看出，层级softmax直接将计算复杂度降低到O(logN)，不过还得额外使用O(N)的空间存储每条路径上的标签向量。

**对于sikp-gram**，和上述过程完全类似，只不过输出时中心词的词向量，然后按照层级softmax去找到所有上下文词的路径，对每条路径求得损失，然后相加得到总的损失函数。分别对各个路径上的节点向量和中心词向量求导得到梯度，然后使用随机梯度上升更新参数和词向量。

**另外一种优化方式为Negative Sampling：**

负采样目的是**提高训练速度**并**改善所得词向量的质量**。

对于CBOW来说，窗口内中心词就是正样本，其它词就是负样本，采样一些负样本词，其实就是在正样本和这些负样本的词集合中做二分类问题，每一个词都对应一个可学习参数，让词向量乘以这个参数经过sigmoid函数生成概率，做二分类损失（正的对数熵），然后将这些损失加起来，最大化该损失，使用随机梯度上升更新词向量和可学习参数。

对于skip-gram来说，窗口内中心词的上下文就是正样本，针对每个正样本都采样一些负样本，同样做二分类损失，将所有正样本对应负样本集合的损失相加，最大化该损失函数，使用随机梯度上升更新词向量和可学习参数。

> 负采样方法：希望频率越高的词被选为负采样概率较大，频率低的词概率较小，即是一个带权采样问题。
>
> 首先计算每个词的词频概率，即词频除以总词数。根据词频概率在0-1区间上做一个非等距划分，每一段都对应一个词的词频概率，可能是1/10，1/14，1/8......，假设共有N段。然后再在0-1区间做一个M段的等距划分，M远大于N，除去首尾的坐标，还有1~M-1个坐标，把这些坐标对应到N段的非等距划分中，即每个坐标对应一个词，建立这样的index2word的table。这样我们只需要在[1,M-1]随机抽取一个整数i，从table中直接对应到词，就可以完成词采样了。
>
> ![image-20220515230023870](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220515230023870.png)

### Q.7 图像数据不足处理方法

> 模型所提供的信息有两种：1. 数据本身蕴含的信息；2. 模型在形成过程中（构造，学习，推理），人们所提供的先验信息。

训练数据不足也可以通过模型和数据上进行处理：

1. 从模型角度，主要是为了缓解数据不足带来的过拟合：比如简化模型，添加约束（L1/L2）正则，集成学习，dropout参数设置等；
2. 从数据角度，主要是进行数据增强（扩充），对于图像来说，（1）物理上可以旋转，平移，缩放，裁剪，填充，翻转等；（2）添加噪声扰动（椒盐，高斯噪声）；（3）颜色变换；（4）改变亮度，清晰度，对比度，锐度等；（5）也可以在特征空间进行变换，使用上采样算法等；
3. 此外，可以借助生产模型生产数据，或者使用迁移学习（与训练微调，知识蒸馏出小模型）来做较少数据的分类或回归任务。



## 模型评估

针对分类，回归，排序，序列预测等不同的ML问题，评估指标也有所不同。

### Q1. 准确率的局限性

> **问题：**Hulu的奢侈品广告主们希望把广告定向投放给奢侈品用户。Hulu通过第三方的数据管理平台（Data Management Platform，DMP）拿到了一部分奢侈品用户的数据，并以此为训练集和测试集，训练和测试奢侈品用户的分类模型。该模型的分类准确率超过了95%，但在实际广告投放过程中，该模型还是把大部分广告投给了非奢侈品用户，这可能是什么原因造成的？

准确率的计算公式为：$Accuracy=\frac{n_{correct}}{n_{total}}$。**如果在训练时准确率高，测试时对于某些类准确率低**，可能是由于训练集类别不均衡造成的。对于一个二分类问题，当99%为负样本时，那么分类器将所有样本都预测为负样本，准确率也有99%。对于这种情况，可以使用更为有效的平均准确率（每个类别下样本的准确率的算数平均值）来评价模型。

当然对于这种问题也不仅限于指标：

（1）过拟合和欠拟合

（2）测试集和训练集划分不合理

（3）线下评估与线上测试样本分布存在差异等等。

### Q2. 精确率与召回率

> **问题：**Hulu提供视频的模糊搜索功能，搜索排序模型返回的Top 5的精确率非常高，但在实际使用过程中，用户还是经常找不到想要的视频，特别是一些比较冷门的剧集，这可能是哪个环节出了问题呢？

对于排序问题，需要指定一个阈值，让模型判断样本是否为正样本，而这个阈值往往是不确定的，往往采样TopN返回结果的Precision（精确率）和Recall（召回率）来衡量模型排序的性能，即认定TopN的结果就是模型认定的正样本。
$$
Precision=\frac{分类正确的正样本个数}{分类器判定为正样本个数} \\
Recall=\frac{分类正确的正样本个数}{所有正样本的个数}
$$
如果TopN中Precision很高，说明模型返回的结果质量很高。然而Recall的值却不一定很高，比如N=5，而正样本有100个，就算Top5全是正样本，而recall也只有5%。

所以实际情况下要选取不同的TopN，结合precision和recall来判断模型好坏，即P-R曲线：

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220515233308591.png" alt="image-20220515233308591" style="zoom:50%;" />

关于P-R曲线的知识点：

（1）P-R曲线是阈值从高到底计算precision和recall构成的平滑曲线，每个点都对于一个阈值下的precision和recall值。

（2）阈值高时，说明TOPN小，那么模型更谨慎的判定正样本，一般精确率会比较高，而相应的召回率小；阈值低时，说明TOPN大，模型会放宽心判断正样本，可能导致精确率有所下降，由于N增大，观测的样本更多，召回率会增大。
