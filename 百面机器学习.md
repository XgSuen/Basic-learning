# 百面机器学习

## 特征工程

结构数据：类似数据库表，有特定类型：数值型和类别型

非结构数据：文本，图像，音频，视频等

### Q1. 特征归一化

**做法**：将所有特征都统一到一个大致相同的数值区间内；

**目的：**消除量纲影响，使得不同指标之间具有可比性；

**方法：**

1. 线性函数归一化（Min-Max scaling），映射到[0,1]范围：
   $$
   X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
   $$
   

2. 零均值归一化（Z-score Normalization），映射到均值为0，方差为1的分布：
   $$
   z=\frac{x-\mu}{\sigma}
   $$

此外，特征归一化可以使得模型学习速率更快，适合梯度下降型的学习算法，对于决策树这种使用信息增益的算法不适用。

### Q2. 类别型特征处理

1. 序号编码：按照一定顺序编号，保留了大小关系
2. one-hot编码：将每个类别编码成类别数量规模大小的向量，在所处位置为1，其余位置为0.（1）稀疏向量节省空间；（2）配合特征选择降维。
3. 二进制编码：先转化为类别ID，再将其转换为二进制编码。维度低，节省空间。

### Q3. 组合特征

提高复杂关系的拟合能力，将一阶离散特征两两组合，构成高阶组合特征。

比如两个类别特征，每个类别有两个取值，那么组合后会变成2x2=4的高阶特征。

### Q4. 有效的找到高阶组合特征

可以采用基于决策树的特征组合方式，从根节点到叶节点的每条路径都可以看成是一种特征组合方式。

> 关于组合特征和组合高阶特征，比如在广告点击预测任务中，找到各个属性（年龄，性别，是否付费等等）相互组合方式对应是否点击的结果来找到行之有效的组合特征，避免过拟合

### Q5. 文本表示模型

1. 词袋模型和TF-IDF

   将每篇文章或每段文本表示成一个向量，先将其**分词**，其长度就是文章或者文本所包含的词数量，向量的值就是对应位置词的权重，可以通过TF-IDF来求得：
   $$
   TF-IDF(t,d)=TF(t,d)\times IDF(t)
   $$
   其中$TF(t,d)$是单词 t 在文档 d 中出现的频率，$IDF(t)$是逆文档频率，衡量单词 t 对表达语义所起的重要性：$IDF(t)=\frac{文章总数}{log(包含单词t的文章总数+1)}$.也就是说，单词t在所有文章中出现次数越多，越通用，就越不重要。

   有时，连续的词被分开含义会发生巨大变化，所以可以将连续出现的 n 个词组成词组（n-gram）也作为一个单独的特征放到向量表示中去。还会对单词进行词干抽取（word stemming），因为词性虽然变化但含义相同，将多种形态的同个词处理为统一的词干形式。

2. 主题模型（Topic model）

   从文本库发现有代表性的主题，得到每个主题上词的分布，从而计算文章的主题分布。

3. 词嵌入模型（Word Embedding）

   将词向量化，将每个词映射为低维空间上的一个稠密向量。

> 在浅层的机器学习模型中，好的特征工程可以带来算法效果的显著提升。
>
> 深度学习模型提供了一种自动特征工程的方式，模型中每个隐层都可以任务对应不同抽象层次的特征。