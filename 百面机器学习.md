# 百面机器学习

## 特征工程

结构数据：类似数据库表，有特定类型：数值型和类别型

非结构数据：文本，图像，音频，视频等

### Q1. 特征归一化

**做法**：将所有特征都统一到一个大致相同的数值区间内；

**目的：**消除量纲影响，使得不同指标之间具有可比性；

**方法：**

1. 线性函数归一化（Min-Max scaling），映射到[0,1]范围：
   $$
   X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
   $$
   
2. 零均值归一化（Z-score Normalization），映射到均值为0，方差为1的分布：
   $$
   z=\frac{x-\mu}{\sigma}
   $$

此外，特征归一化可以使得模型学习速率更快，适合梯度下降型的学习算法，对于决策树这种使用信息增益的算法不适用。

### Q2. 类别型特征处理

1. 序号编码：按照一定顺序编号，保留了大小关系
2. one-hot编码：将每个类别编码成类别数量规模大小的向量，在所处位置为1，其余位置为0.（1）稀疏向量节省空间；（2）配合特征选择降维。
3. 二进制编码：先转化为类别ID，再将其转换为二进制编码。维度低，节省空间。

### Q3. 组合特征

提高复杂关系的拟合能力，将一阶离散特征两两组合，构成高阶组合特征。

比如两个类别特征，每个类别有两个取值，那么组合后会变成2x2=4的高阶特征。

### Q4. 有效的找到高阶组合特征

可以采用基于决策树的特征组合方式，从根节点到叶节点的每条路径都可以看成是一种特征组合方式。

> 关于组合特征和组合高阶特征，比如在广告点击预测任务中，找到各个属性（年龄，性别，是否付费等等）相互组合方式对应是否点击的结果来找到行之有效的组合特征，避免过拟合

### Q5. 文本表示模型

1. 词袋模型和TF-IDF

   将每篇文章或每段文本表示成一个向量，先将其**分词**，其长度就是文章或者文本所包含的词数量，向量的值就是对应位置词的权重，可以通过TF-IDF来求得：
   $$
   TF-IDF(t,d)=TF(t,d)\times IDF(t)
   $$
   其中$TF(t,d)$是单词 t 在文档 d 中出现的频率，$IDF(t)$是逆文档频率，衡量单词 t 对表达语义所起的重要性：$IDF(t)=\frac{文章总数}{log(包含单词t的文章总数+1)}$.也就是说，单词t在所有文章中出现次数越多，越通用，就越不重要。

   有时，连续的词被分开含义会发生巨大变化，所以可以将连续出现的 n 个词组成词组（n-gram）也作为一个单独的特征放到向量表示中去。还会对单词进行词干抽取（word stemming），因为词性虽然变化但含义相同，将多种形态的同个词处理为统一的词干形式。

2. 主题模型（Topic model）

   从文本库发现有代表性的主题，得到每个主题上词的分布，从而计算文章的主题分布。

3. 词嵌入模型（Word Embedding）

   将词向量化，将每个词映射为低维空间上的一个稠密向量。

> 在浅层的机器学习模型中，好的特征工程可以带来算法效果的显著提升。
>
> 深度学习模型提供了一种自动特征工程的方式，模型中每个隐层都可以任务对应不同抽象层次的特征。

### Q6. word2vec

词嵌入模型word2vec分为CBOW和Skip-gram两种网络结构，CBOW是通过窗口的上下文预测中心词的生产概率，Skip-gram是通过窗口的中心词预测上下文的生成概率。他们都由输入层，映射层和输出层组成。单词通过one-hot编码表示。

![image-20220514231706189](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220514231706189.png)

**对于CBOW**，给定一个窗口，通过上下文来预测中心词。即先将中心词的上下文单词的初始词向量相加，然后通过变换映射到词库大小的维度，在这个N维的输出向量上使用softmax计算对应每个词库的概率，然后使用N分类交叉熵损失函数更新窗口内的词向量。通常情况下，这个N会非常大，而计算softmax函数要求挨个词计算它们的概率，即复杂度为O(N)，导致速度非常慢。

一种优化方式是Hierachical（层级）softmax：

![image-20220514232432032](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20220514232432032.png)

层级softmax只需要计算O(logN)次，大大减少了计算复杂度。它的大致过程为：

（1）首先统计N个词的词频，**根据词频建立Huffman树**，其中满足左孩子的词频大于右孩子，**左孩子编码为1，右孩子编码为0**。叶子节点表示所有词，而从根节点通往某个词的路径中所经过的节点都有属于自己的参数向量。（2）然后将窗口上下文词向量的和作为输入，找到根节点到中心词的路径，**根据Huffman编码将每条路径打上标签，0/1，从根节点开始进行二分类**，即向量和与当前节点代表的参数相乘，经过sigmoid函数生成概率值，然后与当前路径编码的标签做二分类损失函数，指导遍历完此路径，到达根节点，将所有损失相加，**最大化该损失和**。（3）然后采用**随机梯度上升法更新词向量**，即每取一个样本（窗口），就对目标函数中所有的参数（路径节点向量）做一次更新。因此要更新两组关键参数：路径中非叶节点的向量参数和窗口内中心词的上下文词向量。让损失函数分别对这两个参数求导得到梯度，再让这些参数加上梯度乘以学习率即可实现梯度上升更新参数。由于我们的输入是词向量和，对每个词向量更新时一般就直接将对词向量和梯度贡献到每个上下文词向量中。

可以看出，层级softmax直接将计算复杂度降低到O(logN)，不过还得额外使用O(N)的空间存储每条路径上的标签向量。

**对于sikp-gram**，和上述过程完全类似，只不过输出时中心词的词向量，然后按照层级softmax去找到所有上下文词的路径，对每条路径求得损失，然后相加得到总的损失函数。分别对各个路径上的节点向量和中心词向量求导得到梯度，然后使用随机梯度上升更新参数和词向量。



### Q.7 图像数据不足处理方法

> 模型所提供的信息有两种：1. 数据本身蕴含的信息；2. 模型在形成过程中（构造，学习，推理），人们所提供的先验信息。

训练数据不足也可以通过模型和数据上进行处理：

1. 从模型角度，主要是为了缓解数据不足带来的过拟合：比如简化模型，添加约束（L1/L2）正则，集成学习，dropout参数设置等；
2. 从数据角度，主要是进行数据增强（扩充），对于图像来说，（1）物理上可以旋转，平移，缩放，裁剪，填充，翻转等；（2）添加噪声扰动（椒盐，高斯噪声）；（3）颜色变换；（4）改变亮度，清晰度，对比度，锐度等；（5）也可以在特征空间进行变换，使用上采样算法等；
3. 此外，可以借助生产模型生产数据，或者使用迁移学习（与训练微调，知识蒸馏出小模型）来做较少数据的分类或回归任务。

