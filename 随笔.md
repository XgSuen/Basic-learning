# 随笔

### 1. Transformer中的PostNorm和PreNorm

![img](https://img-blog.csdnimg.cn/20191216111932859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk0NzE1Ng==,size_16,color_FFFFFF,t_70)

[https://blog.csdn.net/weixin_37947156/article/details/103559399]: 图片来自CSDN

PreNorm：$x_{t+1} = x_t+F(Norm(x_t))$

PostNorm: $x_{t+1}=Norm(x_t+F(x_t))$

看了苏神的解释，大致意思是：

PreNorm更好训练，它无形地增加了模型的宽度而降低了模型的深度，因此效果不如PostNorm；

PostNorm更难训练，效果好，它每Norm一次就削弱一次恒等分支的权重，所以Post Norm反而是更突出残差分支的，对参数敏感，需要使用warm up等技巧训练。

> [为什么Pre Norm的效果不如Post Norm？](https://spaces.ac.cn/archives/9009)

### 2. Warm up

预热学习率（warm up）可以帮助模型有效训练，一开始从一个小的学习率让模型趋于稳定，然后再回复学习率继续训练。

可能是因为：（1）训练开始阶段，权重迅速改变，即开始模型并没有接触所有的数据，如果学习率较大，可能会对当前数据过拟合。而如果模型看过所有数据以后，再调大学习率，在模型有先验知识的情况下就不容易学偏。当模型趋于稳定之后，学习率如果还是很大可能会在局部最优点附近跳跃，那么此时降低学习率可以使得模型接近最优点。

（2）mini-batch 如果数据分布方差大，那么会使得学习剧烈波动，学习到的权重不稳定，学习初期明显，后期缓解。

- 有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳
- 有助于保持模型深层的稳定性

> https://www.zhihu.com/question/338066667